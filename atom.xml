<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jzlikewei.github.io/</id>
    <title>LiBai的记事本</title>
    <updated>2019-08-12T17:37:43.133Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jzlikewei.github.io/"/>
    <link rel="self" href="https://jzlikewei.github.io//atom.xml"/>
    <logo>https://jzlikewei.github.io//images/avatar.png</logo>
    <icon>https://jzlikewei.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, LiBai的记事本</rights>
    <entry>
        <title type="html"><![CDATA[leetcode题解 No846.一手顺子]]></title>
        <id>https://jzlikewei.github.io//post/leetcode-no846-Hand-of-Straights</id>
        <link href="https://jzlikewei.github.io//post/leetcode-no846-Hand-of-Straights">
        </link>
        <updated>2019-05-21T06:57:24.000Z</updated>
        <content type="html"><![CDATA[<p>很没有难度的一道题目，读懂题目后，按照对应的规则来走一遍就OK了。</p>
<ol>
<li>将手牌排序后，重叠的牌我们叠在一起（如下所示）：</li>
</ol>
<pre><code>#样例数据1
数量：(1)(2)(2)(1)(1)(1)(1)           
牌面：[1][2][3][4][6][7][8]
</code></pre>
<ol start="2">
<li>每次按照顺序从排好序的手牌里取w张牌，放到一个【组】里</li>
</ol>
<ul>
<li>如果组内牌不连续，返回false</li>
<li>如果取不够w张牌，返回false</li>
</ul>
<pre><code>#取出 1,2,3后
数量：(0)(1)(1)(1)(1)(1)(1)           
牌面：[1][2][3][4][6][7][8]
</code></pre>
<ol start="3">
<li>
<p>​ 调整下手牌，使手牌继续有序且“重叠”，如果手牌非空，goto Step 2</p>
<pre><code>#调整下空位,现在手牌中的第一张是2了
数量：(1)(1)(1)(1)(1)(1)           
牌面：[2][3][4][6][7][8]
</code></pre>
<p>​</p>
</li>
</ol>
<p>很容易写出一个双重循环的解法：</p>
<pre><code>while 手牌非空{
  for(i = 0;i&lt;w;++i){
    从手牌中取牌
  }
  调整手牌，移除空位
}
</code></pre>
<p>这部分的时间复杂度是O(N) N = hand.length</p>
<p><code>这里虽然是双重循环，但是时间复杂度并不是O(Hand.length*w),因为，每张牌我们都至多使用一次，所以是O(N)</code></p>
<p>手牌排序然后重叠，这样一个操作的时间复杂度是<code>O(N*log(n))</code>，所以整体的时间复杂度是<code>O(N*log(n))</code></p>
<pre><code class="language-ruby">def is_n_straight_hand(hand, w)
  return false if hand.length % w != 0
  elements = {}
  hand.each { |i| elements[i] = elements[i].to_i + 1}
  elements = elements.to_a
  elements.sort! { |x,y| x[0] &lt;=&gt; y[0] }
  until elements.empty?
    group = []
    w.times do |i|
      return false if i &gt;= elements.size
      group &lt;&lt; elements[i][0]
      elements[i][1] -= 1
      if group.size &gt; 1
        return false if group[-1] != group[-2] + 1
      end
    end
    while !elements.empty? and elements[0][1] == 0
      elements.shift
    end
  end
  true
end
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[老司机教你看妹子——你的第一个知乎爬虫]]></title>
        <id>https://jzlikewei.github.io//post/zhihu-spider-tutorial-one</id>
        <link href="https://jzlikewei.github.io//post/zhihu-spider-tutorial-one">
        </link>
        <updated>2017-07-21T06:57:24.000Z</updated>
        <content type="html"><![CDATA[<p>#老司机教你看妹子——你的第一个知乎爬虫</p>
<p>###序</p>
<p>你看着知乎上一个个的爆照贴，怎样能把这些图片都保存下来呢。</p>
<p>少年你听说过爬虫么，会写python么</p>
<p>“啊，老师啊，有没有什么可以批量保存小姐姐的功能啊”</p>
<p>“emmmm，老师我python装好了，3.7”</p>
<p>走着</p>
<p>####把大象放进冰箱里</p>
<p>“老师我知道，第一步打开冰箱门，第二第把大象放进去，第三步关上冰箱门”，这个冷笑话你已经听了无数遍了，已经学会抢答了。</p>
<p>保存小姐姐也只需要三步：</p>
<pre><code>1.打开网页
2.找到图片
3.保存
</code></pre>
<p>你默默的打开了搜索引擎：“如何用python打开网页”，然后一堆urllib之类的内容宛如天书，你又默默的关闭了网页。</p>
<p>“老师，有没有什么人类也能懂得方法来打来网页啊”</p>
<p><a href="https://docs.python-requests.org/en/master/">Requests: HTTP for Humans</a></p>
<p>“老师，他们还有<a href="http://docs.python-requests.org/zh_CN/latest/index.html">中文版文档</a>诶”</p>
<p>####STEP 1</p>
<p>安装了库，读了一会文档后，你写出了第一步需要的代码：</p>
<p>Python 代码（python3）：</p>
<pre><code>import requests
url='https://www.zhihu.com/question/20399991'
r = requests.get(url)
text= r.text
</code></pre>
<p>你心怀期待的跑了一下：<br>
<img src="https://jzlikewei.github.io//post-images/1565631255906.png" alt=""></p>
<p>“老师，我把知乎的服务器搞挂了，你看500了！”</p>
<p>真的这样么，刚刚不好好的么？你换了浏览器结果发现浏览器里还好好的啊。</p>
<p>很明显，知乎服务器把你给骗了，而且它还很恶意的把你的连接挂了很久,刚刚那段代码会很久很久才跑出结果。</p>
<p>“难道说，它发现我是爬虫了？！”</p>
<p>对的</p>
<p>“怎么发现的呢？”</p>
<p><strong>浏览器在发送请求的时候，通常会带上一个User-Agent，这个字符串里通常会包含操作系统信息和浏览器的一些信息。</strong></p>
<p>所以网站才会知道你用的是Android还是iOS，Windows还是Mac，iPad还是iPhone，Chrome还是IE。</p>
<p>“哦，那我们现在的user-agent是啥呢”</p>
<p>默认的是&quot;python-requests/1.2.0&quot;</p>
<p><img src="https://jzlikewei.github.io//post-images/1565631273937.jpg" alt=""></p>
<p>你打开了Chrome，找到了开发者工具，在Network一栏里，点开一条网络请求，从Request Headers里找到自己浏览器的UA</p>
<pre><code>import requests
header = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}
url='https://www.zhihu.com/question/20399991'
r = requests.get(url,headers=header)
text= r.text
</code></pre>
<p>这样看起来就是一个正常的Chrome浏览器在访问页面了</p>
<p>这下就拿到结果了。</p>
<p>####STEP 2</p>
<p>你轻轻的在交互式窗口里输入了 <code>print text</code>，结果涌现出来的东西把你吓了一跳。</p>
<p>“这一坨HTML要怎么办啊，哪些是图片呢？我要去怎么解析他们啊”</p>
<p>“python how to parse html” 你在搜索栏里打下这行字<br>
<img src="https://jzlikewei.github.io//post-images/1565631301803.png" alt=""></p>
<p>你熟练的点开了 Stack Overflow的那个连接，准备复制黏贴，高票回答让你去用一个Beautiful Soup之类的东西。</p>
<p>你默默的打开了煤气灶，准备煲汤...</p>
<p><img src="https://jzlikewei.github.io//post-images/1565631313589.jpg" alt=""></p>
<p>“我只想下载个图片啊，老师能不能简单点，你说话的方式简单点...”</p>
<p>你当然可以用类似Beautiful Soup 之类的库去解析HTML的页面结构，但是我们需求没那么复杂(等汤煲好天都亮了)，我们就简单的写个正则来匹配下吧。</p>
<p>一个典型的图片链接地址是这样的，<code>https://pic1.zhimg.com/v2-dfad0e20695394c5fe79bfa5ec9dd170_b.jpg</code>(贴心的知乎做了全站https)，观察下发现，用<code>https://</code>开头以<code>.jpg</code>结尾，中间任意字符，同时，为了避免匹配到类似<code>https://a/c.jpg&lt;/img&gt;https://d/f.jpg</code>这样的结果，我们需做最短匹配。</p>
<p>最后拿到就拿到了这样一个式子<code>https://[^\s]*?\.jpg</code></p>
<p>“那只有jpg，还有png\jpeg\gif呢？”</p>
<p>多匹配几次喽...</p>
<pre><code>jpg = re.compile(r'https://[^\s]*?\.jpg')
jpeg = re.compile(r'https://[^\s]*?\.jpeg')
gif = re.compile(r'https://[^\s]*?\.gif')
png = re.compile(r'https://[^\s]*?\.png')
imgs=[]
imgs+=jpg.findall(text)
imgs+=jpeg.findall(text)
imgs+=gif.findall(text)
imgs+=png.findall(text)
</code></pre>
<p>#####STEP 3</p>
<p>“下载图片我会！把图片链接写到txt文件里，然后用迅雷下载！”</p>
<p>放过迅雷吧，写一个花不了多久的。</p>
<pre><code>def download(url):
    req = requests.get(url)
    if req.status_code == requests.codes.ok:
        name = url.split('/')[-1]
        f = open(&quot;./&quot;+name,'wb')
        f.write(req.content)
        f.close()
        return True
    else:
        return False
</code></pre>
<p>“然后是一个for循环，挨个下载，然后还能记录下哪些连接出错了”，你已经轻车熟路了。</p>
<pre><code>errors = []
for img_url in imgs:
    if download(img_url):
        print(&quot;download :&quot;+img_url)
    else:
        errors.append(img_url)
print(&quot;ERROR URLS:&quot;)
print(errors)
</code></pre>
<p>让我们运行一下，有图片了！</p>
<p><img src="https://jzlikewei.github.io//post-images/1565631375119.png" alt=""><br>
以及一坨错误的URL：</p>
<p><img src="https://jzlikewei.github.io//post-images/1565631386055.png" alt=""></p>
<p>#####STEP 5 检查</p>
<p>“为什么有两份呢，还有些莫名其妙的小图片。”<br>
你把这些图快速的X了一遍，啊不，浏览了一遍，发现重复的图片基本以_r和_b结尾，而_r是原图，而以_l,_xs,_is,结尾的都是头像，这些没啥好看的。</p>
<p>“老师我发现了，错误的url除了<code>https://pic4.zhimg.com/***_{size}.jpg</code>之类以外，还有带转义符的，转义符导致我们的正则匹配出了问题，不是一个合法的url.”</p>
<p>“还有啊，这个图是不是少了点，这个问题可是有一个有706个回答呢”</p>
<p><code>***_{size}.jpg</code>和一些转义符都是因为这些内容本来是要经过JS在浏览器渲染的，图少的原因则是因为，答案的加载是有分页的，网页上拉倒后面能看到“更多”的按钮。</p>
<p>稍稍修改下正则表达式，我们只要原图。</p>
<pre><code>jpg = re.compile(r'https://[^\s]*?_r\.jpg')
jpeg = re.compile(r'https://[^\s]*?_r\.jpeg')
gif = re.compile(r'https://[^\s]*?_r\.gif')
png = re.compile(r'https://[^\s]*?_r\.png')
</code></pre>
<p>给download函数加一个文件夹的参数，把图片存在文件夹里，然后封装一个函数，就可以一次批量下载多个URL了。</p>
<pre><code>urls=['https://www.zhihu.com/question/22212644','https://www.zhihu.com/question/22212644',
      'https://www.zhihu.com/question/31983868','https://www.zhihu.com/question/20399991']
for url in urls :
    fetch(url)
	
</code></pre>
<p>“具体要怎么写啊老师，诶老师你干嘛去啊...”</p>
<p>自己写，爆照贴有新回复了，我看图去...</p>
<p>####Final</p>
<pre><code>import os
import re
import requests
def download(folder,url):
    if not os.path.exists(folder):
        os.makedirs(folder)
    req = requests.get(url)
    if req.status_code == requests.codes.ok:
        name = url.split('/')[-1]
        f = open(&quot;./&quot;+folder+'/'+name,'wb')
        f.write(req.content)
        f.close()
        return True
    else:
        return False

header = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}

errs=[]

def fetch(url):
    r = requests.get(url,headers=header)
    text= r.text
    imgs=[]
    jpg = re.compile(r'https://[^\s]*?_r\.jpg')
    jpeg = re.compile(r'https://[^\s]*?_r\.jpeg')
    gif = re.compile(r'https://[^\s]*?_r\.gif')
    png = re.compile(r'https://[^\s]*?_r\.png')

    imgs+=jpg.findall(text)
    imgs+=jpeg.findall(text)
    imgs+=gif.findall(text)
    imgs+=png.findall(text)


    errors = []

    folder = url.split('/')[-1]
    for img_url in imgs:
        if download(folder,img_url):
            print(&quot;download :&quot;+img_url)
        else:
            errors.append(img_url)
    return errors

urls=['https://www.zhihu.com/question/22212644','https://www.zhihu.com/question/29814297',
      'https://www.zhihu.com/question/31983868','https://www.zhihu.com/question/20399991']
for url in urls :
    print(url)
    errs+=fetch(url)

print(&quot;ERROR URLS:&quot;)
print(errs)
</code></pre>
<p><img src="https://jzlikewei.github.io//post-images/1565631445586.jpg" alt=""></p>
<h4 id=""></h4>
<p>“老师，那我们要怎样才能获取到完整的回答啊，以及怎么把小姐姐们和照片对应起来啊”</p>
<p>那是下次的内容啦</p>
<p>“那老师下次去哪里找你啊”</p>
<p>关注一发喽</p>
]]></content>
    </entry>
</feed>